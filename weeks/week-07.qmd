---
title: "Week 7: Decide with Numbers"
subtitle: "Thresholds, Trade-offs, and Decision Frameworks"
format: 
  revealjs:
    theme: simple
    transition: slide
    slide-number: true
    incremental: true
    width: 1600
    height: 1200
    center: true
---

## Today's Agenda

::: {.callout-tip}
## Interactive Session Alert!
Today involves **team challenges**, **case study battles**, and **threshold setting competitions**. Come ready to defend your decisions and challenge others!
:::



- **Statistics Refresher**: Hypothesis testing, t-tests, and sample sizes (super simple!)

- **Warm-up**: Peer Review #1 feedback integration

- **Case Study Battle**: Real-world threshold disasters vs. successes

- **Team Challenge**: Competitive decision framework development

- **Hot Seat**: Counter-argument rapid-fire session

- **Change Log Workshop**: Document your learning evolution

---

## Quick Statistics Refresher 
### (Because A3 Showed We Need This!)

---

### Slide 1: Hypotheses - Like Design Claims You Can Test

**üèóÔ∏è Think of a hypothesis like saying: "My design will work better than existing solutions"**

::: {.columns}

::: {.column width="50%"}
**In Architecture:**


- "This office layout will improve productivity"

- "Better lighting will reduce eye strain"

- "Open spaces will increase collaboration"
:::

::: {.column width="50%"}
**In Statistics:**


- **H0 (Null)**: "No difference" (old design = new design)

- **H1 (Alternative)**: "There IS a difference" (new design > old design)
:::

:::

**üéØ Your job**: Collect enough evidence to **reject H0** (prove the old way is NOT as good)

::: {.callout-note}
## Analogy: Court Trial

- **H0** = "Defendant is innocent" (default assumption)

- **H1** = "Defendant is guilty" (what you're trying to prove)

- You need **overwhelming evidence** to reject innocence
:::

**Real Example**: "Biophilic design increases worker satisfaction by >0.5 points on 5-point scale"

- **H0**: Biophilic = Traditional (no difference)

- **H1**: Biophilic > Traditional (satisfaction increase ‚â•0.5)

---

### Slide 2: T-tests - Your Evidence Judge

**üéØ T-test = "Is this difference REAL or just random luck?"**

::: {.columns}

::: {.column width="60%"}
**What it does:**


- Compares two groups (old vs. new design)

- Accounts for natural variation in data

- Gives you a **p-value** (probability it's just luck)

**The Magic Number**: p < 0.05

- "Less than 5% chance this is random"

- "Pretty confident the difference is real"
:::

::: {.column width="40%"}
**Architecture Example:**


- Test 20 people in old office

- Test 20 people in new office  

- Productivity scores: 6.2 vs 7.1

- **Question**: Is 0.9 difference real?

- **T-test says**: p = 0.032 ‚úÖ

- **Conclusion**: Yes, probably real!
:::

:::

::: {.callout-warning}
## Analogy: Coin Flipping
If you flip 10 heads in a row, is the coin rigged? 

- **Probability of this by chance**: 1/1024 = 0.001

- **Way less than 0.05** ‚Üí Coin is probably rigged!
:::

**Common Mistake**: "My 3 test users liked it better" ‚â† statistical significance!

---

### Slide 3: Sample Size - How Many People Do You Actually Need?

**üé≤ The Goldilocks Problem: Not too few, not too many, just right**

::: {.columns}

::: {.column width="50%"}
**Too Small (n < 10):**

- ‚ùå Can't detect real differences

- ‚ùå Results change drastically with one more person

- ‚ùå No statistical power

**Too Large (n > 100):**

- ‚úÖ Very reliable BUT...

- üí∞ Expensive and time-consuming

- ü§Ø May detect differences that don't matter practically
:::

::: {.column width="50%"}
**Just Right (n = 20-50 for most studies):**

- ‚úÖ Can detect meaningful differences

- ‚úÖ Manageable cost and time

- ‚úÖ Reliable enough for decisions

**Quick Rules of Thumb:**


- **Pilot studies**: 10-15 people minimum

- **User testing**: 20-30 people

- **Survey research**: 30+ people

- **Big differences**: Fewer people needed

- **Small differences**: More people needed
:::

:::

::: {.callout-tip}
## Analogy: Restaurant Reviews

- **3 reviews**: Not enough to trust

- **30 reviews**: Pretty reliable picture

- **300 reviews**: Overkill for most decisions, but very reliable
:::

**Power Calculation Tools**: G*Power (free software) - tells you exactly how many you need!

**A3 Reality Check**: Most of you needed 20-30 people minimum, not 8-12! üìä

---

## Peer Review Lightning Round

**üî• 2-Minute Partner Share** 
*Find someone you haven't worked with yet*

**Round 1**: What was the **most surprising** piece of feedback you received? 
**Round 2**: What was the **most actionable** suggestion?
**Round 3**: What feedback did you **respectfully disagree** with and why?

::: {.callout-warning}
## Class Poll Time!
Raise your hand if peer feedback made you:

- üîÑ Change your method significantly

- üéØ Refocus on different stakeholders  

- üìä Redesign your data visualization

- ü§î Question your original claim
:::

**Now let's see what patterns we can spot across all projects...**

---

## The Great Threshold Disasters of Architecture

---

### Case Study Battle: When Good Evidence Goes Bad

**üèóÔ∏è The Pruitt-Igoe Housing Complex (1954-1976)**

- **Evidence**: "High-density housing improves urban efficiency"

- **Threshold**: "500+ units per acre is optimal"

- **Reality**: Social dysfunction, crime, eventual demolition

- **Missing**: Social interaction thresholds, community scale limits

**‚ùÑÔ∏è The Champlain Towers South Collapse (2021)**

- **Evidence**: "Concrete structures last 50+ years with basic maintenance"

- **Threshold**: "Visual inspection every 10 years is sufficient"

- **Reality**: Catastrophic failure after 40 years

- **Missing**: Accelerated corrosion thresholds in coastal environments

::: {.callout-note}
## Discussion Prompt
**Think-Pair-Share (5 mins)**: What architectural decision in your own project could go catastrophically wrong if you set the wrong threshold? What's your "Pruitt-Igoe" scenario?
:::

---

### Threshold Reality Check: The Hong Kong Context

**üèôÔ∏è Local Case Study: Hong Kong's Plot Ratio Limits**

- **Current threshold**: 5:1-8:1 in urban areas

- **Evidence basis**: Transportation capacity, infrastructure load

- **Controversy**: Does this create housing shortage vs. livability?

**üí° Your Turn - Rapid Fire Questions:**

*Stand up and move to different corners of the room:*

**Corner A**: "My threshold is based on regulations/codes"
**Corner B**: "My threshold is based on user satisfaction data" 
**Corner C**: "My threshold is based on economic analysis"
**Corner D**: "My threshold is... honestly, I'm not sure yet"

*Now defend your corner choice to the other groups! (3 mins each)*

---

## Setting Appropriate Thresholds

---

### Regulatory and Standards-Based

**Building code compliance:**

- Minimum performance requirements

- Safety factors and margins

- Local vs. international standards

**Green building standards:**

- LEED, BREEAM, Green Mark thresholds

- Point systems and credit requirements

- Market expectations and norms

---

### Performance Benchmarking

**Peer building comparison:**

- Industry average performance levels

- Best-in-class examples

- Worst acceptable performance

**User satisfaction benchmarks:**

- Survey response thresholds (e.g., 4.0/5.0 satisfaction)

- Complaint rates and frequencies

- Productivity impact measurements

---

### Stakeholder-Specific Thresholds

**Developer criteria:**

- ROI minimums and payback periods

- Risk tolerance and market positioning

- Competitive advantage requirements

**User-focused criteria:**

- Comfort and satisfaction minimums

- Health and safety thresholds

- Accessibility and inclusion requirements

---

## Counter-Argument Analysis

---

### Anticipating Objections

**"Too expensive"**

- Life-cycle cost analysis

- Non-monetary benefit quantification

- Financing and incentive options

- Risk cost of not implementing

**"Unproven technology"**

- Precedent examples and case studies

- Gradual implementation strategies

- Pilot testing and monitoring plans

- Backup and contingency options

---

### Alternative Explanations

**"Results could be due to other factors"**

- Sensitivity analysis across variables

- Control for confounding factors

- Multiple validation approaches

- Transparent limitation acknowledgment

**"Context may not transfer"**

- Boundary condition analysis

- Similar context identification

- Adaptation strategies

- Local validation recommendations

---

## Trade-Off Analysis Frameworks

---

### Multi-Criteria Decision Analysis

**Identify competing objectives:**

- Energy performance vs. cost

- User satisfaction vs. maintenance

- Sustainability vs. aesthetics

- Privacy vs. collaboration

**Weight criteria by stakeholder priority:**

- Survey stakeholders for preference ranking

- Use revealed preferences from past decisions

- Apply regulatory or market requirements

---

### üìä Live Decision Matrix Showdown

**Real-time class voting on trade-off weights!**

*Use your phones to vote on Mentimeter/Kahoot:*

**Question 1**: For a Hong Kong residential developer, rank importance (1-5):

- Energy performance: ?

- Construction cost: ?

- User satisfaction: ?

- Maintenance ease: ?

- Resale value: ?

**Live Results Display**:

| Alternative | Energy (X%) | Cost (Y%) | User Sat (Z%) | Maintenance (W%) | **Total** |
|-------------|-------------|-----------|---------------|------------------|-----------|
| High-tech   | 9           | 3         | 8             | 6                | **?**     |
| Standard    | 6           | 8         | 7             | 8                | **?**     |
| Budget      | 4           | 10        | 5             | 9                | **?**     |

**Plot twist**: Now recalculate with class-voted weights!

::: {.callout-important}
## Reality Check
How different are the results with class weights vs. your individual assumptions? What does this tell us about stakeholder engagement?
:::

---

### üéØ The Scenario Planning War Games

**üé≤ Choose Your Adventure Format**

*Each student rolls a die to get their scenario assignment:*

**üåü Best Case (Roll 1-2)**: "Everything Goes Perfect"

- All assumptions optimistic, technology works flawlessly

- Stakeholders embrace change, funding flows freely

- **Your mission**: Sell this vision BUT acknowledge it's optimistic

**‚ö° Worst Case (Roll 3-4)**: "Murphy's Law Strikes"

- Technology fails, budgets get cut, regulations change

- Users resist, maintenance breaks down, economy crashes

- **Your mission**: Plan for disaster WITHOUT being paralyzed

**üéØ Most Likely (Roll 5-6)**: "Reality Check"

- Some things work, some don't, typical implementation friction

- Mixed stakeholder response, moderate budget pressure

- **Your mission**: Balance optimism with pragmatism

**The Challenge**: Present your scenario in 90 seconds with:
1. What happens in your scenario
2. How your recommendation changes
3. What contingency plans you need

**Class votes**: Which scenario team was most convincing?

::: {.callout-warning}
## Professional Reality
Real clients want to know you've thought through all scenarios. Being overly optimistic OR pessimistic kills credibility.
:::

---

## The Decision Framework Speed Dating

---

### üíñ Musical Chairs Decision Development

**Round 1: Threshold Speed Dating (15 minutes)**

*Students rotate every 3 minutes*

**Station A**: "Threshold Reality Check" 

- Share your key threshold with partner

- Partner plays devil's advocate: "That's too high/low because..."

- Defend or revise your threshold

**Station B**: "Objection Preparation" 

- List 3 likely objections to your recommendation

- Partner adds 2 more objections you didn't think of

- Brainstorm rapid responses

**Station C**: "Trade-off Mapping"

- Identify your biggest trade-off dilemma

- Partner suggests creative ways to minimize the trade-off

- Test different stakeholder weighting scenarios

**Station D**: "Evidence Stress Test"

- Present your strongest evidence

- Partner identifies the weakest link

- Develop backup evidence or acknowledge limitations

**Station E**: "Implementation Reality"

- Describe how your recommendation would actually happen

- Partner identifies practical barriers

- Problem-solve implementation strategies

---

### üí• The Framework Peer Review Arena

**Battle Royale Format**: Groups of 4 students

**Round 1: The Threshold Challenge** (5 minutes)

- Each person shares one key threshold

- Group collectively attacks each threshold: "But what if..."

- Threshold survives = 1 point

- Threshold gets improved = 2 points

- Threshold gets completely revised = 3 points (best learning!)

**Round 2: The Objection Olympics** (5 minutes)

- Each person presents their worst-case objection scenario

- Group brainstorms rapid-fire solutions

- Most creative solution wins the round

**Round 3: The Trade-off Twist** (5 minutes)

- Each person presents their biggest trade-off dilemma

- Group suggests "third option" solutions that minimize trade-offs

- Vote on most innovative compromise

**Scoring**: Track points across all rounds. Winning group shares best insights with class!

::: {.callout-note}
## Reflection Moment
**Quick whip-around**: What's one insight from peer review that **changed your mind** about something?
:::

---

## The Learning Evolution Showcase

---

### üí´ Change Log Speed Sharing

**Before we document, let's celebrate the learning!**

**üîÑ The Evolution Stations** (3 minutes each station)

**Station 1: "My Biggest Pivot"**

- Share the most significant change you made to your approach

- What triggered this change?

- How do you feel about abandoning your original idea?

**Station 2: "My Stubborn Stand"** 

- Share something you REFUSED to change despite feedback

- Why did you stick to your guns?

- Are you still confident in this decision?

**Station 3: "My Surprise Discovery"**

- What did you learn that completely surprised you?

- How did this discovery change your perspective?

- What would you have done differently knowing this from the start?

**Station 4: "My Method Makeover"**

- What tool/method did you completely overhaul?

- What forced this change?

- Is the new approach actually better?

::: {.callout-tip}
## Documentation Strategy
After sharing, you'll have great material for your Change Log! Use these conversations to write more authentic, specific entries.
:::

---

### üìù Change Log Competition: Most Honest Documentation

**The Challenge**: Write the most authentic, insightful change log entries

**Categories for Voting**:

- üèÜ **Most Honest Failure**: Best documentation of something that didn't work

- üí° **Best Plot Twist**: Most surprising mid-course discovery

- üå± **Greatest Growth**: Most impressive skill/thinking development

- ü§ù **Best Collaboration**: How peer feedback created breakthrough

**Format Template**:
**Date | Change | Trigger | Impact | Lesson Learned**

**Example Entries**:

*Oct 15 | Abandoned fancy simulation for simple Excel model | Spent 20 hours on software that crashed constantly | Actually got better insights from simpler approach | Complexity ‚â† Quality*

*Oct 22 | Completely changed target stakeholder from architects to facility managers | Realized architects don't actually make operational decisions | Research became 10x more relevant and actionable | Always validate your assumptions about decision-makers*

**Sharing Format**: Read your best entry aloud (1 min max). Class votes in each category!

::: {.callout-important}
## Professional Reality
Change logs are becoming standard in consulting and research. Clients love seeing transparent documentation of learning and adaptation - it builds trust!
:::

---

### Reflection Questions

1. **What's different about your approach now vs. Week 1?**
2. **Which changes were forced vs. chosen?**
3. **What would you do differently if starting over?**
4. **How has your confidence in the results evolved?**
5. **What skills have you developed through adaptation?**

---

## Professional Decision-Making Integration

---

### Architecture Firm Decision Processes

**Design phase integration:**

- Concept development evidence requirements

- Design development validation needs

- Construction document specification support

**Client communication strategies:**

- Technical evidence translation

- Risk and benefit communication

- Alternative evaluation presentation

---

### Regulatory and Compliance Context

**Building approval processes:**

- Code compliance demonstration

- Performance-based design justification

- Alternative solution documentation

**Post-occupancy validation:**

- Performance monitoring and verification

- User feedback integration

- Continuous improvement processes

---

## A4 Development Strategy

---

### Integration Across Components

**Research Brief recommendations:**

- Clear thresholds and decision criteria

- Counter-argument acknowledgment

- Trade-off analysis and stakeholder guidance

**Practice Case actionability:**

- Specific implementation recommendations

- Timeline and resource requirements

- Risk mitigation strategies

**Object Card clarity:**

- Key threshold visually prominent

- Decision recommendation clear

- Confidence level indicated

---

### Timeline to Object Fair

**Week 8:** Mentor consultations and workflow documentation
**Week 9:** Practice Case development and narrative refinement  
**Week 10:** Draft defense and feedback integration
**Week 11:** Final polish and presentation preparation
**Week 12:** Object Fair presentations and final submission

---

## Your Mission for This Week

---

### üéØ The Week 7 Challenge Checklist

**Level 1: Survival Mode** ‚úÖ
1. **Change Log #1** - Document your evolution (use today's insights!)
2. **Threshold Setting** - Get specific numbers with solid justification
3. **Counter-argument Prep** - List and respond to 5 likely objections

**Level 2: Excellence Mode** üéñÔ∏è
4. **Stakeholder Validation** - Actually ask a real stakeholder about your thresholds
5. **Sensitivity Analysis** - Test how robust your recommendations are
6. **Implementation Roadmap** - How would this actually happen?

**Level 3: Ninja Mode** ü•∑
7. **Alternative Scenario Planning** - What if key assumptions are wrong?
8. **Peer Collaboration** - Help a classmate strengthen their weakest argument
9. **Professional Polish** - Make one figure publication-ready

::: {.callout-warning}
## Accountability Check
**Next week starts with a 2-minute "threshold defense" - be ready to justify your numbers with confidence!**
:::

**Which level are you committing to? Say it out loud to the person next to you!**

---

### Week 8 Preview

**Light Workflow Help**

- Documenting processes for reproducibility

- Automation ethics and appropriate tool use

- Individual mentor consultations (30-45 min each)

- Workflow optimization strategies

---

## Your Evidence-Based Design Superpowers

::: {.callout-important}
## What You Conquered Today

üéØ **Threshold Setting Mastery** - You can now set defensible performance standards

üî• **Counter-Argument Immunity** - You're prepared for professional skepticism

‚öñÔ∏è **Trade-off Navigation** - You can help stakeholders understand difficult choices

üìù **Learning Documentation** - You can show professional growth and adaptability

ü§ù **Peer Collaboration** - You've strengthened your work through collective intelligence
:::

### üîÆ Fortune Cookie Wisdom

*Pull a random "evidence-based design fortune" from the hat before you leave:*


- "A threshold without justification is just a guess with confidence"

- "The best counter-argument comes from your harshest critic - embrace them"

- "Trade-offs reveal values - make yours explicit"

- "Document failures as carefully as successes - both teach"

- "Peer feedback is free consulting - use it wisely"

**Your fortune for the week**: _______________

::: {.callout-tip}
## Exit Ticket
**Before you leave, tell the person next to you**: What's the ONE thing from today that will change how you approach your project this week?
:::

---

*Next week: Workflow documentation and mentor consultations - plus the legendary "Method Reproducibility Challenge"! üèÜ*